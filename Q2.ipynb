{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP/JTHRHgW8ESQN2Is/456",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a51745/RL-b-11/blob/main/Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Monte Carlo Policy Evaluation & Control (Colab)\n",
        "# ===============================================\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------\n",
        "# GridWorld Environment\n",
        "# -------------------------\n",
        "class GridWorld:\n",
        "    ACTIONS = [0, 1, 2, 3]   # 0:UP, 1:RIGHT, 2:DOWN, 3:LEFT\n",
        "    DIRS = {0:(-1,0), 1:(0,1), 2:(1,0), 3:(0,-1)}\n",
        "    ARROWS = {0:'↑', 1:'→', 2:'↓', 3:'←'}\n",
        "\n",
        "    def __init__(self, H=4, W=4, walls=None, terminals=None, step_cost=-0.04):\n",
        "        self.H, self.W = H, W\n",
        "        self.walls = set(walls or [])\n",
        "        self.terminals = dict(terminals or {})\n",
        "        self.step_cost = step_cost\n",
        "        self.states = [(r,c) for r in range(H) for c in range(W) if (r,c) not in self.walls]\n",
        "\n",
        "    def in_bounds(self, r, c):\n",
        "        return 0 <= r < self.H and 0 <= c < self.W\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s in self.terminals\n",
        "\n",
        "    def step(self, s, a):\n",
        "        \"\"\"Return next_state, reward, done\"\"\"\n",
        "        if self.is_terminal(s):\n",
        "            return s, 0, True\n",
        "        r, c = s\n",
        "        dr, dc = self.DIRS[a]\n",
        "        nr, nc = r + dr, c + dc\n",
        "        if (not self.in_bounds(nr,nc)) or ((nr,nc) in self.walls):\n",
        "            ns = (r,c)\n",
        "        else:\n",
        "            ns = (nr,nc)\n",
        "        reward = self.step_cost\n",
        "        if ns in self.terminals:\n",
        "            reward += self.terminals[ns]\n",
        "        return ns, reward, self.is_terminal(ns)\n",
        "\n",
        "    def reset(self):\n",
        "        # Start in a random non-terminal, non-wall state\n",
        "        return random.choice([s for s in self.states if not self.is_terminal(s)])\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Monte Carlo: Policy Evaluation\n",
        "# -------------------------\n",
        "def mc_policy_evaluation(env, policy, episodes=5000, gamma=0.99):\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        # Generate an episode\n",
        "        episode = []\n",
        "        s = env.reset()\n",
        "        while True:\n",
        "            a = policy[s]\n",
        "            ns, r, done = env.step(s, a)\n",
        "            episode.append((s,a,r))\n",
        "            s = ns\n",
        "            if done: break\n",
        "\n",
        "        # First-visit MC\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "            if s not in visited:\n",
        "                returns_sum[s] += G\n",
        "                returns_count[s] += 1\n",
        "                V[s] = returns_sum[s] / returns_count[s]\n",
        "                visited.add(s)\n",
        "    return V\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Monte Carlo Control (ε-greedy)\n",
        "# -------------------------\n",
        "def mc_control_epsilon_greedy(env, episodes=10000, gamma=0.99, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(len(env.ACTIONS)))\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    def policy_fn(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.choice(env.ACTIONS)\n",
        "        else:\n",
        "            return np.argmax(Q[state])\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        # Generate an episode\n",
        "        episode = []\n",
        "        s = env.reset()\n",
        "        while True:\n",
        "            a = policy_fn(s)\n",
        "            ns, r, done = env.step(s, a)\n",
        "            episode.append((s,a,r))\n",
        "            s = ns\n",
        "            if done: break\n",
        "\n",
        "        # Compute returns\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = gamma * G + r\n",
        "            if (s,a) not in visited:\n",
        "                returns_sum[(s,a)] += G\n",
        "                returns_count[(s,a)] += 1\n",
        "                Q[s][a] = returns_sum[(s,a)] / returns_count[(s,a)]\n",
        "                visited.add((s,a))\n",
        "\n",
        "    # Derive greedy policy\n",
        "    policy = {}\n",
        "    for s in env.states:\n",
        "        if env.is_terminal(s):\n",
        "            policy[s] = None\n",
        "        else:\n",
        "            policy[s] = np.argmax(Q[s])\n",
        "    return Q, policy\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Visualization Helpers\n",
        "# -------------------------\n",
        "def print_policy(env, policy, title=\"Policy\"):\n",
        "    print(title)\n",
        "    for r in range(env.H):\n",
        "        row = []\n",
        "        for c in range(env.W):\n",
        "            s = (r,c)\n",
        "            if s in env.walls:\n",
        "                row.append(\"■\")\n",
        "            elif s in env.terminals:\n",
        "                row.append(f\"T({env.terminals[s]:.1f})\")\n",
        "            elif policy[s] is None:\n",
        "                row.append(\"•\")\n",
        "            else:\n",
        "                row.append(GridWorld.ARROWS[policy[s]])\n",
        "        print(\" \".join(f\"{x:>6}\" for x in row))\n",
        "\n",
        "def plot_values(env, V, title=\"State-Value Function\"):\n",
        "    A = np.zeros((env.H, env.W))\n",
        "    A[:] = np.nan\n",
        "    for s in env.states:\n",
        "        A[s] = V[s]\n",
        "    plt.figure(figsize=(5,5))\n",
        "    im = plt.imshow(A, cmap=\"coolwarm\", interpolation='nearest')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            if (r,c) in env.walls:\n",
        "                txt = '■'\n",
        "            elif (r,c) in env.terminals:\n",
        "                txt = f\"T\\n{env.terminals[(r,c)]:.1f}\"\n",
        "            else:\n",
        "                txt = f\"{A[r,c]:.2f}\"\n",
        "            plt.text(c, r, txt, ha='center', va='center', fontsize=9)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Example Run\n",
        "# -------------------------\n",
        "walls = {(1,1)}\n",
        "terminals = {(0,3): +1.0, (1,3): -1.0}\n",
        "env = GridWorld(H=4, W=4, walls=walls, terminals=terminals, step_cost=-0.04)\n",
        "\n",
        "# Random policy evaluation\n",
        "random_policy = {s: np.random.choice(env.ACTIONS) if not env.is_terminal(s) else None for s in env.states}\n",
        "V_mc = mc_policy_evaluation(env, random_policy, episodes=5000)\n",
        "plot_values(env, V_mc, \"MC Policy Evaluation (Random Policy)\")\n",
        "\n",
        "# Monte Carlo Control\n",
        "Q_mc, policy_mc = mc_control_epsilon_greedy(env, episodes=10000, epsilon=0.1)\n",
        "print_policy(env, policy_mc, \"MC Control Policy\")\n"
      ],
      "metadata": {
        "id": "CoEFpX-m_Z13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}